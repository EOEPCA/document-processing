{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":"<p>The Processing Building Block is designed to provide capabilities for the hosted execution of processing workflows.</p> <p>These workflows are defined either as OGC Application Packages or as openEO Process Graphs. </p> <p>The key to successful execution of user-defined processing (such as algorithms or models) within a platform is the availability of appropriately curated input data that meets the specific needs of the processing workflow.</p>"},{"location":"#about-the-processing-building-block","title":"About the Processing Building Block","text":"<p>The main goal of the Processing Building Block is to offer a consistent and reliable data foundation upon which developers of processing workflows can depend. </p> <p>To achieve this, a generic data curation approach is favoured, allowing the needs of various processing workflows to be met across multiple platforms. This approach eliminates the necessity for each individual platform to develop its own unique data integration solutions.</p> <p>Included in the Processing Building Block components, the Processing Engine offers a versatile and scalable solution for managing and executing EO data processing workflows, with the added capability of integrating with various backend execution environments and supporting data discovery processes.</p> <p>The Processing Engine functions as follows:</p> <ul> <li> <p>Service API Provider: The Processing Engine offers a service API through which processing workflows are submitted for execution. This API is the primary interface for users to interact with the Processing BB, facilitating the submission and management of processing tasks.</p> </li> <li> <p>Support for openEO and Data Discovery: In the case of openEO, the Processing Engine also encompasses functionalities related to data discovery. This integration signifies that the engine not only manages processing tasks but also aids in identifying and retrieving relevant datasets for processing.</p> </li> <li> <p>Integration with Backend Execution Environments: The architecture of the Processing Building Block is designed to support an extensible set of backend execution environments capable of running the processing workflows. This extensibility means that the Processing Engine can be adapted to various deployment scenarios, enhancing its versatility and applicability.</p> </li> <li> <p>Role in Workflow Management: The Processing Engine plays a central role in the workflow management ecosystem of the EOEPCA project. It acts as the gateway through which processing requests are channelled, managed, and executed, interfacing with other components like Processing Runners, Data Sources, and Processing Clients.</p> </li> </ul> <p>For effective implementation, the Processing BB is envisioned to be modular in design, making it adaptable and extensible. This modular structure enables it to meet diverse processing requirements and integrate smoothly with different platform architectures.</p>"},{"location":"#capabilities","title":"Capabilities","text":"<p>The relevant use cases for the Processing Building Block are centred around the architecture and components that enable efficient and adaptable processing workflows. </p> <p>It comprises the following key components, each addressing specific aspects of processing in EO:</p> <ul> <li> <p>Processing Engine: Serves as the core component, providing a service API for submitting processing workflows. In some instances, it also includes functionalities for data discovery. This engine is crucial for initiating and managing processing tasks.</p> </li> <li> <p>Processing Runner: Offers the execution environment for the processing workflows. It is designed to be plugged into the Processing Engine for specific deployment scenarios, providing the necessary computational resources and environment for processing tasks.</p> </li> <li> <p>Processing Data Source: Integrates with data sources to make datasets available as inputs for processing workflows. This component ensures that the required data is accessible and in the correct format for processing, bridging the gap between data storage and processing functionalities.</p> </li> <li> <p>Processing Client: A library with bindings for common programming languages such as Python, facilitating programmatic access to the Processing Engine\u2019s services via its API. This client enables developers and users to interact with the Processing Engine, submit jobs, and manage workflows programmatically.</p> </li> <li> <p>Processing Development Tooling: Includes tools that support the development, visualisation, and packaging of processing workflows. These tools are essential for the creation and testing of new processing algorithms and models, enhancing the usability and accessibility of the Processing Building Block.</p> </li> </ul> <p>The Processing Building Block includes multiple Processing Engines and Runners, as it is designed to support a range of processing workflow approaches and execution environments:</p>"},{"location":"clients/web-editor/","title":"Introduction","text":"<p>The documentation for the <code>&lt;BB&gt;</code> building block is organised as follows\u2026</p> <ul> <li>Introduction   Introduction to the BB - including summary of purpose and capabilities.</li> <li>Getting Started   Quick start instructions - including installation, e.g. of a local instance.</li> <li>Design   Description of the BB design - including its subcomponent architecture and interfaces.</li> <li>Usage   Tutorials, How-tos, etc. to communicate usage of the BB.</li> <li>Administration   Configuration and maintenance of the BB.</li> <li>API   Details of APIs provided by the BB - including endpoints, usage descriptions and examples etc.</li> </ul>"},{"location":"clients/web-editor/#about-bb","title":"About <code>&lt;BB&gt;</code>","text":"<p>The Processing Editor provides a GUI for creating and executing processes on EOEPCA backends.</p>"},{"location":"clients/web-editor/#capabilities","title":"Capabilities","text":"<p>Both the openEO API and the OGC API - Processes are supported.</p> <p>The functionalities include:</p> <ul> <li>discovering available resources such as imagery collections and processing functions,</li> <li>combining them into workflows using a GUI graph builder,</li> <li>and submitting those for actual calculation, including job management and visualisation of the results.</li> </ul>"},{"location":"clients/web-editor/#design","title":"Design","text":"<p>The EOEPCA Processing Editor is a clone of the openEO Web Editor, just with a custom design and added functionality regarding OGC API - Processes.</p> <p>It is a dockerised Node.js project serving a Vue.js web app, utilising the openEO JS Client and openEO Vue Components, and leveraging OpenLayers and CodeMirror.</p>"},{"location":"clients/web-editor/configuration/","title":"Configuration","text":""},{"location":"clients/web-editor/configuration/#deployment","title":"Deployment","text":"<p>The deployed version is run from a generic Helm chart in conjunction with a values file managed in ArgoCD which overrides the chart\u2019s template defaults via Kustomize.</p> <p>See the commit of the Helm chart for some comments on how it works: https://github.com/EOEPCA/helm-charts-dev/commit/5dacb2ef81200827ce928613459283400e575efd</p> <p>And look at the comments in these three commits to understand the deployment via ArgoCD: d6613d4, 8203d64, 2a8f611 (Note that it was later amended by commits 50a93e1, 877fbca, 4b9bc4f, a7bd098, 7ff3042.)</p> <p>The most important points to understand how the two play together: The actual deployment work is done by the templates in the Helm chart. But these are just a scaffold, which takes the concrete values from the <code>values.yaml</code> (and <code>Chart.yaml</code> for the metadata). However, in the ArgoCD part, there is another <code>values-openeo-web-editor.yaml</code>, whose values override those from the aforementioned <code>values.yaml</code> of the chart. The idea is that the chart provides defaults, but could be used for different environments which each provide their own values. So the ArgoCD repo also contains \u2018Helm stuff\u2019 \u2013 only the <code>openeo-web-editor.yaml</code> (without the <code>values-</code> prefix) is the \u2018actual\u2019 ArgoCD file.</p> <p>To make changes in the Helm chart take effect, the version number in the <code>Chart.yaml</code> must be bumped, so that the Github action is triggered. For changes in the ArgoCD repo, a simple commit is enough for redeployment.</p> <p>This dashboard shows all running containers of the deployment: https://argocd.develop.eoepca.org/ (login with Github credentials)</p>"},{"location":"clients/web-editor/configuration/#application","title":"Application","text":"<p>The configuration of the application itself is done via the <code>config.js</code>. Additionally, there is a <code>theme.scss</code> for colours/styling.</p> <p>There is also a docs folder in the repo itself, see there for more information e.g. on OIDC authentication.</p>"},{"location":"clients/web-editor/quick-start/","title":"Quick Start","text":"<p>If you just want to use it, simply head to the deployed version: https://editor.develop.eoepca.org/</p> <p>If you want to set it up yourself, there is a Docker image: https://hub.docker.com/r/eoepca/processing-editor</p> <p>The source code is on Github: https://github.com/EOEPCA/processing-editor See there for instructions on how to run it.</p> <p>The deployed version runs on the EOEPCA Kubernetes Cluster through a Helm chart in conjunction with ArgoCD. See the Configuration section of these docs for more information on this.</p>"},{"location":"design/more-design/","title":"More design\u2026","text":"<p>Further elaboration of design\u2026</p>"},{"location":"design/overview/","title":"Architecture","text":"<p>The Processing Building Block includes multiple Processing Engines and Runners, as it is designed to support a range of processing workflow approaches and execution environments:</p> <ul> <li> <p>Processing Engines: The Processing Building Block  is capable of supporting one or more processing workflow approaches. These include:</p> <ul> <li> <p>OGC Application Packages via OGC API Processes (deployment, instantiation and execution).</p> </li> <li> <p>openEO Process Graphs via the openEO Specification.</p> </li> </ul> </li> <li> <p>Processing Runners: The Processing Building Block should supports the execution of processing workflows in various extensible execution environments. Previous projects focused mainly in Docker and Kubernetes, some experiments were also performed in OGC activities for High-Performance Computing (HPC) by GeoLabs and EURAC, and Terradue for Argo Workflows. Other possible environments for Processing Runners include also, but are not limited, to Dask Cluster and Apache Airflow.</p> </li> </ul>"},{"location":"design/processing-engine/oa/","title":"Application Package Processing Engine","text":""},{"location":"design/processing-engine/oa/#introduction-to-the-overall-emsades-architecture","title":"Introduction to the overall EMS/ADES architecture","text":"<p>The OGC initiatives in Testbed-13, Testbed-14, and Testbed-15 have pioneered the development of an advanced architectural framework designed to facilitate the ad-hoc deployment and execution of applications proximate to the data source. This strategic positioning minimizes data transfer, enhancing efficiency between data repositories and application processes.</p> <p>These testbeds laid the foundational design for an architecture that supports the encapsulation, deployment, and execution of Earth Observation Applications across distributed Cloud Platforms. This framework ensures that applications are efficiently integrated and managed within cloud environments, providing scalable solutions for Earth Observation data processing.</p> <p>Building on established OGC standards such as WPS (Web Processing Service) and OWS-Context, these testbed activities have refined the concept of Application Packages. These packages encapsulate data processing applications or workflows, allowing for streamlined deployment and execution across varied Cloud Platforms.</p> <p>The WPS service played a pivotal role by enabling end-user portals and B2B client applications to submit processing parameters, initiate on-demand or systematic data processing requests, and establish data pipelines for retrieving the processed information.</p> <p>Each Application Package contains detailed specifications about the execution units or workflow scripts to be executed, along with configurations for parameterization or customization. This modular approach facilitates the precise tailoring of processing workflows to specific project needs.</p> <p>The architecture articulated in these testbeds integrates the Application Package with key services: the Execution Management Service (EMS) and the Application Deployment and Execution Service (ADES). The EMS orchestrates the deployment and invocation of processing services workflows, leveraging multiple deployment and execution platforms through WPS-T, a transactional extension of WPS. The ADES, in turn, functions as the execution engine, managing the application lifecycle previously configured as a WPS service by the EMS.</p> <p>The EMS orchestrates the application package across the selected ADES platform, managing the deployment and execution of workflow components either locally or remotely. It ensures a streamlined operation from deployment to execution, fostering a responsive and agile processing environment.</p> <p>Primary responsibilities of the ADES include:</p> <ul> <li>Validating and approving application deployment and execution requests from the EMS.</li> <li>Managing the execution of processes within the processing cluster, ensuring robust performance and reliability.</li> <li>Monitoring ongoing process execution to maintain operational integrity and performance.</li> <li>Retrieving and delivering processed results efficiently, ensuring data integrity and accessibility.</li> </ul> <p>Additionally, the ADES handles critical sub-steps such as the staging-in of input data and the staging-out of output data, essential for maintaining a seamless flow in data processing activities. This comprehensive approach not only enhances operational efficiency but also ensures that the architecture can support complex, data-intensive workflows in an optimized manner.</p>"},{"location":"design/processing-engine/oa/#evolution-of-the-application-package","title":"Evolution of the Application Package","text":"<p>The OGC Innovation Program\u2019s Earth Observation Applications Pilot, conducted between December 2019 and July 2020, investigated the software architecture initially developed through OGC Testbeds 13-15. This architecture facilitated the deployment and execution of externally developed applications on Earth Observation (EO) data and processing platforms.</p> <p>The pilot program served as an evaluation of the interoperability frameworks previously established in OGC Testbeds 13 to 15, proving to be a foundational step for conducting maturity assessments within operational environments. During the pilot, further enhancements and precise definitions were crafted, which refined the process for deploying and executing applications across a variety of platforms with nly minimal modifications required.</p> <p>The Pilot validated the strategy of using Docker for secure and efficient application packaging, and HTTP Web APIs or Web Services for managing and executing applications. Moreover, the pilot defined clear application patterns that govern data inputs and outputs, reinforced the use of the Common Workflow Language (CWL) for detailed application description, execution, and workflow construction. It also endorsed the adoption of the SpatioTemporal Asset Catalog (STAC) to serve as a comprehensive data manifest, delineating the necessary inputs and outputs for applications. These advancements collectively enhance the deployment flexibility and interoperability of applications within diverse EO data ecosystems.</p> <p>The conclusion of the OGC Earth Observation Applications Pilot emphasized the need for standardized subsets of the Common Workflow Language (CWL) that all platforms should support. This standardization was reckoned essential to ensure that application developers experience consistency and predictability when deploying their applications across different platforms. As a result, the pilot advocated for the creation of an OGC Best Practice document that would outline the use of CWL within application packages.</p> <p>This proposed Best Practice document would aim to serve as a comprehensive guide, addressing the diverse application design patterns explored during the pilot. It would provide detailed guidelines for the automated generation of CWL, enabling a more streamlined and efficient development process for EO applications. This initiative would seek to enhance the interoperability and usability of Earth Observation data and processing platforms by leveraging CWL\u2019s robust framework for describing and executing workflows, thereby fostering a more unified and effective approach to application development within the geospatial community.</p>"},{"location":"design/processing-engine/oa/#the-common-workflow-language-and-the-ocg-earth-observation-application-packages","title":"The Common Workflow Language and the OCG Earth Observation Application Packages.","text":"<p>CWL was selected as the open standard to support the packaging of Earth Observation applications in several OGC activities and these are depicted below:</p> flowchart LR     subgraph References[\"`**References**`\"]       subgraph Testbed-13         tb13[\"`EP Application Package Engineering Report (17-023)             Application Deployment and Execution Service Engineering Report (17-024)             Cloud Engineering Report (17-035)`\"]     end     subgraph Testbed-14         tb14[\"`Application Package Engineering Report (18-049r1)             ADES &amp; EMS Results and Best Practices Engineering Report (18-050r1)`\"]     end     subgraph Testbed-15         tb15[\"`Catalogue and Discovery Engineering Report (19-020r1)`\"]     end     end     subgraph eoap[\"`**Earth Observation Applications Pilot**`\"]         eoaper[\"`20-042 Terradue Engineering Report                 20-045 CRIM Engineering Report                 20-038 European Union Satellite Centre Engineering Report                 20-043 EOX Consortium Engineering Report                 20-037 Pixalytics Engineering Report                 20-034 Spacebel Engineering Report`\"]         summary-er[\"EO Applications Pilot          Summary Engineering Report (20-073)\"]         eoaper --&gt; summary-er     end     Testbed-13 --&gt; eoap     Testbed-14 --&gt; eoap     Testbed-15 --&gt; eoap     summary-er --&gt; bp[\"`Best Practice for Earth Observation Application Package (20-089)`\"]      bp --&gt; tb16[\"`Testbed-16: Earth Observation Application Packages with Jupyter Notebooks`\"]     bp --&gt; tb17[\"`Testbed 17: COG/Zarr Evaluation Engineering Report`\"]     bp --&gt; tb18[\"`Testbed-18: Testbed-18: Reproducible FAIR Best Practices Engineering Report`\"]"},{"location":"design/processing-engine/openeo/","title":"OpenEO Processing Engine","text":"<p>EOEPCA supports earth observation processing via the openEO standard. We recommend reading up on openEO to understand the basic motivation.</p> <p>In openEO terminology, a processing engine is referred to as a \u2018back-end\u2019. Various open source and proprietary back-ends already exist. A back-end implements the openEO API on top of a specific technology stack. This means that implementations often differ in terms of the capabilities and processes of the openEO API that it implements. </p> <p>Back-ends also offer a number of \u2018collections\u2019 which represent (earth observation) datasets that can be processed on that backend. Hence, data access and processing can be optimized in a specific instance.</p>"},{"location":"design/processing-engine/openeo/#openeo-geotrellis","title":"openEO Geotrellis","text":"<p>The documentation of this component can be found here.</p> <p>It is built on top of the Geotrellis Scala library, utilizing Apache Spark for high performance distributed computing.</p> <p>This backend targets various container orchestrators such as Kubernetes and Apache YARN and can also run standalone in a simple Docker container. For HPC, we recommend to simply orchestrate multiple standalone instances using e.g. SLURM, or explore more complex options.</p> <p>These are some known example deployments that used this backend in a fully operational context:</p> <ul> <li>Terrascope</li> <li>Copernicus DataSpace Ecosystem</li> </ul>"},{"location":"design/processing-engine/openeo/#openeo-xarray","title":"openEO XArray","text":"<p>This backend can be found here.</p> <p>It is built on top of XArray, which relies on Dask for distributed processing. </p> <p>Dask can be deployed in a Kubernetes environment, or standalone.</p>"},{"location":"design/processing-runner/argo/","title":"Argo Workflows Processing Runners","text":"<p>Argo Workflows, with its container-native workflow engine, is adept at managing intricate job dependencies and facilitating the execution of multiple Kubernetes pods in parallel. </p> <p>This makes it an ideal candidate for integration as a Processing Runner in EO platforms, especially given the complex nature of EO data processing, which often involves extensive data manipulation and analysis pipelines.</p> <p>Argo Workflows is distinguished by its ability to support scalable and efficient tasks, accommodating various data artefacts in the process. Its seamless integration with Kubernetes harnesses the power of pods, secrets, and volumes, enhancing its capacity to manage large-scale workflows. This compatibility with Kubernetes infrastructure not only streamlines workflow execution but also ensures that Argo Workflows can leverage the full spectrum of Kubernetes features to optimise processing tasks. </p> <p>Integrating Argo Workflows as a Processing Runner promises several benefits:</p> <ul> <li>Enhanced orchestration capabilities for complex EO data processing tasks.</li> <li>Improved scalability and efficiency, enabling the EO platform to handle larger datasets and more sophisticated processing workflows.</li> <li>Greater flexibility in managing job dependencies, facilitating more complex analyses and machine learning pipelines.</li> </ul> <p>The integration of Argo Workflows as a Processing Runner for Zoo is depicted below.</p> <p></p>"},{"location":"design/processing-runner/dask/","title":"Dask Processing Runner","text":"<p>Dask, a flexible parallel computing library for analytic computing, is especially well-suited for tasks that are common in EO, such as large-scale image processing, data manipulation, and machine learning. </p> <p>The Dask Processing Runner may become an essential component in the landscape of EO data processing, particularly for its role in enhancing the handling and analysis of large-scale, complex datasets and other aspects like:</p> <ul> <li>Handling Large Datasets Efficiently: Dask\u2019s ability to process large datasets that do not fit into the memory of a single machine is a significant advantage. It achieves this by breaking down large computations into smaller, manageable tasks and distributing them across a cluster. This approach is vital for EO data, which can be voluminous and complex.</li> <li>Parallel Processing Capabilities: Dask excels in parallel data processing, allowing for faster analysis and computation. This is crucial in EO where time-sensitive data analysis is often required, such as in environmental monitoring or disaster response scenarios.</li> <li>Scalability and Flexibility: Dask  provides scalability, allowing EO data processing workflows to scale up or down based on the computational needs. This flexibility is important in meeting the varying demands of different EO projects, ensuring efficient use of resources.</li> <li>Integration with Python Ecosystem: Dask integrates with the existing Python data science ecosystem, which is widely used in the EO community. This allows for the use of familiar tools and libraries, thereby reducing the learning curve and enabling more straightforward implementation of processing workflows.</li> </ul> <p>Dask offers a scalable approach to data processing, adept at handling large datasets that exceed the memory limitations of individual machines. It achieves this through intelligent task scheduling and distributed computing. Dask\u2019s compatibility with Python\u2019s data science stack makes it a natural choice for integration into existing EO workflows, which often rely on Python-based tools and libraries.</p> <p>The integration of Dask with Zoo is depicted below.</p> <p></p>"},{"location":"design/processing-runner/docker/","title":"Docker Processing Runner","text":"<p>While the Kubernetes Processing Runner targets the execution of processing workflows on Kubernetes clusters, the Docker Processing Runner targets the execution of processing workflows on computing resources that can rely on container engines such as docker or podman to run the workflow containerized command line tools.</p> <p>Containers are isolated environments that contain all the necessary executables, binary code, libraries, and configuration files required to run an application. This ensures that the application runs consistently across different computing environments, from a developer\u2019s local laptop to high-capacity cloud servers, eliminating the \u201cit works on my machine\u201d problem. Containers are lightweight, making them ideal for creating scalable, efficient, and secure application deployments.</p> <p>The integration of cwltool as the CWL runner with Zoo is depicted below.</p> <p></p>"},{"location":"design/processing-runner/hpc/","title":"High-Performance Computing (HPC) Processing Runner","text":"<p>HPC systems are designed to handle and process large-scale data computations, making them highly suitable for the data-intensive nature of EO processing tasks. The use of HPC can significantly reduce processing time for complex computations, thereby increasing efficiency and throughput. In this context, Slurm emerges as a powerful and flexible workload manager. </p> <p>It enables effective job scheduling, resource allocation, and cluster management, which are crucial for optimising the performance of HPC environments in processing EO data.</p> <p>In the case of the OGC API Processes using Application Packages, the design follows the same model as used for the Kubernetes/Calrissian runner but the CWL runner is Toil since it support Slurm.</p> <p>The design is depicted below:</p> <p></p>"},{"location":"design/processing-runner/kubernetes/","title":"Kubernetes Processing Runner","text":"<p>The role of Kubernetes as a Processing Runner in the EO platforms is becoming increasingly vital because of its orchestration capabilities that play a crucial role in managing containerized applications and workloads. </p> <p>Its use as a Processing Runner is pivotal in achieving high levels of scalability, reliability, and efficiency in data processing tasks.</p> <p>Kubernetes facilitates the management of complex processing tasks by efficiently handling container deployment, scaling, and operation. This is particularly important in the context of EO, where processing workloads can be highly variable and resource-intensive. As the demand for processing EO data grows, the need for a scalable and flexible infrastructure becomes critical.</p> <p>Kubernetes addresses this need by enabling dynamic horizontal scaling of resources, ensuring that processing capabilities can be ramped up or down based on real-time requirements, thereby optimising resource utilisation and reducing operational costs.</p> <p>The Kubernetes processing runner relies on Calrissian, which is a CWL (Common Workflow Language) runner designed to operate on Kubernetes clusters. It allows users to execute CWL workflows in a cloud-native environment, taking advantage of Kubernetes for orchestrating and scaling complex computational workflows. Calrissian offers a powerful platform for executing CWL workflows on Kubernetes, providing scalability, resource efficiency, and the ability to handle complex, data-intensive computational tasks.</p> <p>The integration of Calrissian as the CWL runner for Kubernetes with Zoo is depicted below.</p> <p></p>"},{"location":"getting-started/more-getting-started/","title":"More getting started\u2026","text":"<p>Further elaboration of Getting Started\u2026</p> <p>Coming soon</p>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Quick start instructions - including installation, e.g. of a local instance.</p> <p>Coming soon</p>"},{"location":"oa/admin/configuration/","title":"Configuration","text":"<p>How the BB is configured - with examples etc.</p>"},{"location":"oa/admin/maintenance/","title":"Maintenance","text":"<p>Administrative and remedial activities to be performed on a running BB instance</p>"},{"location":"oa/api/endpoint-specification/","title":"Specification","text":""},{"location":"oa/api/usage/","title":"Usage","text":"<p>API usage descriptions and examples.</p>"},{"location":"oa/getting-started/more-getting-started/","title":"More getting started\u2026","text":"<p>Further elaboration of Getting Started\u2026</p> <p>Coming soon</p>"},{"location":"oa/getting-started/quick-start/","title":"Quick Start - Boostrap a deployment on minikube","text":""},{"location":"oa/getting-started/quick-start/#requirements","title":"Requirements","text":"<p>Before you begin, make sure you have the following tools installed and set up on your local environment:</p>"},{"location":"oa/getting-started/quick-start/#skaffold","title":"Skaffold","text":"<p>Skaffold is used to build, push, and deploy your application to Kubernetes. </p> <p>You can install it by following the instructions here.</p>"},{"location":"oa/getting-started/quick-start/#helm","title":"Helm","text":"<p>Helm is a package manager for Kubernetes, enabling you to manage Kubernetes applications easily. </p> <p>You can install it by following the steps here.</p>"},{"location":"oa/getting-started/quick-start/#minikube","title":"Minikube","text":"<p>Minikube runs a local Kubernetes cluster, ideal for development and testing. </p> <p>You can install it by following the guide here.</p> <p>Start your minikube instance with:</p> <pre><code>minikube start\n</code></pre>"},{"location":"oa/getting-started/quick-start/#optional-requirements","title":"Optional requirements","text":""},{"location":"oa/getting-started/quick-start/#kubectl","title":"Kubectl","text":"<p>Kubectl is a command-line tool for interacting with Kubernetes clusters. It allows you to manage and inspect cluster resources. While not strictly required, it\u2019s highly recommended for debugging and interacting with your Kubernetes environment.</p> <p>You can install it by following the instructions here.</p>"},{"location":"oa/getting-started/quick-start/#openlens","title":"OpenLens","text":"<p>OpenLens is a graphical user interface for managing and monitoring Kubernetes clusters. It provides a visual way to interact with resources. </p> <p>While it\u2019s optional, it can significantly improve your workflow. You can download it here.</p>"},{"location":"oa/getting-started/quick-start/#add-the-helm-repositories","title":"Add the helm repositories","text":"<pre><code>helm repo add localstack https://helm.localstack.cloud\nhelm repo add zoo-project https://zoo-project.github.io/charts/\n</code></pre>"},{"location":"oa/getting-started/quick-start/#checking-the-requirements","title":"Checking the requirements","text":"<p>After installing these tools, ensure they are available in your terminal by running the following commands:</p> <pre><code>skaffold version\nhelm version\nminikube version\n</code></pre> <p>If all commands return a version, you\u2019re good to go!</p>"},{"location":"oa/getting-started/quick-start/#clone-the-repository","title":"Clone the repository","text":"<pre><code>git clone https://github.com/eoap/dev-platform-eoap/\ncd dev-platform-eoap/\n</code></pre>"},{"location":"oa/getting-started/quick-start/#run-the-ogc-api-processes-with-zoo-project-module","title":"Run the OGC API Processes with ZOO Project module","text":"<p>Bootstrap the deployment with: <pre><code>cd ogc-api-processes-with-zoo\nskaffold dev\n</code></pre> Wait for the deployment to stablize (1-2 minutes) and the open your browser on the link printed, usually http://127.0.0.1:8000.</p> <p>The typical output is:</p> <pre><code>No tags generated\nStarting deploy...\nHelm release zoo-project-dru not installed. Installing...\nNAME: zoo-project-dru\nLAST DEPLOYED: Tue Oct 22 07:44:52 2024\nNAMESPACE: eoap-zoo-project\nSTATUS: deployed\nREVISION: 1\nNOTES:\n1. Get the application URL by running these commands:\n  export POD_NAME=$(kubectl get pods --namespace eoap-zoo-project -l \"app.kubernetes.io/name=zoo-project-dru,app.kubernetes.io/instance=zoo-project-dru\" -o jsonpath=\"{.items[0].metadata.name}\")\n  export CONTAINER_PORT=$(kubectl get pod --namespace eoap-zoo-project $POD_NAME -o jsonpath=\"{.spec.containers[0].ports[0].containerPort}\")\n  echo \"Visit http://127.0.0.1:8080 to use your application\"\n  kubectl --namespace eoap-zoo-project port-forward $POD_NAME 8080:$CONTAINER_PORT\nHelm release eoap-zoo-project-coder not installed. Installing...\nNAME: eoap-zoo-project-coder\nLAST DEPLOYED: Tue Oct 22 07:44:54 2024\n...\n...\n - eoap-zoo-project:statefulset/zoo-project-dru-postgresql: Waiting for 1 pods to be ready...\n - eoap-zoo-project:statefulset/zoo-project-dru-rabbitmq: Waiting for 1 pods to be ready...\n - eoap-zoo-project:statefulset/zoo-project-dru-postgresql is ready. [5/7 deployment(s) still pending]\n - eoap-zoo-project:deployment/eoap-zoo-project-localstack is ready. [4/7 deployment(s) still pending]\n - eoap-zoo-project:statefulset/zoo-project-dru-rabbitmq is ready. [3/7 deployment(s) still pending]\n - eoap-zoo-project:deployment/zoo-project-dru-zoofpm is ready. [2/7 deployment(s) still pending]\n - eoap-zoo-project:deployment/zoo-project-dru-zookernel is ready. [1/7 deployment(s) still pending]\n - eoap-zoo-project:deployment/code-server-deployment is ready.\nDeployments stabilized in 51.073 seconds\nPort forwarding service/code-server-service in namespace eoap-zoo-project, remote port 8080 -&gt; http://localhost:8000\nPort forwarding service/zoo-project-dru-service in namespace eoap-zoo-project, remote port 80 -&gt; http://localhost:8080\nNo artifacts found to watch\nPress Ctrl+C to exit\nWatching for changes...\n</code></pre> <p>Open the link http://localhost:8000 to access the Code Server</p> <p>Open the link http://localhost:8080/swagger-ui/oapip/ to access the Zoo Project swagger</p>"},{"location":"oa/getting-started/quick-start/#follow-the-tutorial","title":"Follow the tutorial","text":"<p>This tutorial is designed for developers, scientists, and Earth observation enthusiasts who want to get acquainted with the ZOO-Project OGC API Processes implementation to deploy and run Application Packages as web services</p> <ul> <li>Documentation available at: https://eoap.github.io/ogc-api-processes-with-zoo/</li> <li>Repository available at: https://github.com/eoap/ogc-api-processes-with-zoo/</li> </ul>"},{"location":"oa/usage/howtos/","title":"How-Tos","text":"<p>Coming soon!</p>"},{"location":"oa/usage/tutorials/","title":"Tutorials","text":""},{"location":"oa/usage/tutorials/#where-to-search-for-tutorials","title":"Where to search for tutorials","text":"<p>The Github organization EOAP includes a set of repositories with training material regarding the Earth Observation Application Package.</p> <p>This set of repositories aims to:</p> <ul> <li>help developers create Earth Observation (EO) applications using the Common Workflow Language (CWL)</li> <li>provide an overview of the CWL, its key concepts</li> <li>how to build a CWL-based EO application using practical examples in field guides</li> </ul>"},{"location":"oa/usage/tutorials/#application-package-and-cwl-as-a-solution-for-earth-observation-portability","title":"Application Package and CWL as a solution for Earth Observation portability","text":"<p>This documentation provides an introduction to CWL as a solution for the portability of EO applications</p> <p>Documentation available at: https://eoap.github.io/cwl-eoap/</p> <p>Repository available at: https://github.com/eoap/cwl-eoap</p>"},{"location":"oa/usage/tutorials/#understanding-stac-for-inputoutput-data-modelling-in-earth-observation-applications","title":"Understanding STAC for input/output data modelling in Earth Observation Applications","text":"<p>Documentation and notebooks for understanding the role of STAC as input/output data manifests in Earth Observation applications and a hands-on with real-life scenarios</p> <p>Documentation available at: https://eoap.github.io/stac-eoap/</p> <p>Repository available at: https://github.com/eoap/stac-eoap</p>"},{"location":"oa/usage/tutorials/#quickwin-a-simple-application-package-for-getting-started","title":"Quickwin - A simple Application Package for getting started","text":"<p>This tutorial is designed for developers, scientists, and Earth observation enthusiasts who want to get started with the EO Application Package.</p> <p>Documentation available at: https://eoap.github.io/quickwin</p> <p>Repository available at: https://github.com/eoap/quickwin </p>"},{"location":"oa/usage/tutorials/#mastering-earth-observation-application-packaging-with-cwl","title":"Mastering Earth Observation Application Packaging with CWL","text":"<p>This tutorial is designed for developers, scientists, and Earth observation enthusiasts who want to enhance their skills in creating and sharing EO Application Packages.</p> <p>Documentation available at: https://eoap.github.io/mastering-app-package</p> <p>Repository available at: https://github.com/eoap/mastering-app-package</p>"},{"location":"oa/usage/tutorials/#quickwin-an-application-package-with-inline-python-code","title":"Quickwin - An Application Package with inline Python code","text":"<p>This tutorial is designed for developers, scientists, and Earth observation enthusiasts who want to get started with the EO Application Package.</p> <p>Repository available at: https://github.com/eoap/quickwin-inline-code</p>"},{"location":"oa/usage/tutorials/#open-and-reproducible-eo-application-package","title":"Open and reproducible EO Application Package","text":"<p>Many cloud-based solutions for workflows in EO are available to users today, but only few support reproducibility or comply with FAIR data principles. </p> <p>This short tutorial demonstrates how EO Application Packages meet these requirements.</p> <p>Documentation available at: https://eoap.github.io/open-reproducible-app-package</p> <p>Repository available at: https://github.com/eoap/open-reproducible-app-package</p>"},{"location":"oa/usage/tutorials/#inference-with-the-eo-application-package","title":"Inference with the EO Application Package","text":"<p>This tutorial addresses the packaging of the inference using an ONNX model. </p> <p>Documentation available at: https://eoap.github.io/inference-eoap</p> <p>Repository available at: https://github.com/eoap/inference-eoap</p>"},{"location":"oa/usage/tutorials/#references","title":"References","text":""},{"location":"oa/usage/tutorials/#common-workflow-language","title":"Common Workflow Language","text":"<p>Common Workflow Language User Guide, a guide to introduce you to writing workflows using the Common Workflow Language (CWL) open standards</p> <p>User guide available at: https://www.commonwl.org/user_guide/</p> <p>Specification and standards</p> <ul> <li>Common Workflow Language specification: https://www.commonwl.org/specification/</li> <li>Common Workflow Language Standards, v1.2: https://www.commonwl.org/v1.2/</li> <li>Common Workflow Language Standards, v1.1: https://www.commonwl.org/v1.1/</li> <li>Common Workflow Language Standards, v1.0.2: https://www.commonwl.org/v1.0/</li> </ul>"},{"location":"oa/usage/tutorials/#ogc-documents","title":"OGC documents","text":"<ul> <li>OGC 20-089r1 Best Practice for Earth Observation Application Package: https://docs.ogc.org/bp/20-089r1.html</li> <li>OGC 20-073: OGC Earth Observation Applications Pilot: Summary Engineering Report: http://docs.opengeospatial.org/per/20-073.html</li> <li>OGC 20-042: OGC Earth Observations Applications Pilot: Terradue Engineering Report: http://docs.opengeospatial.org/per/20-042.html</li> </ul>"},{"location":"oa/usage/tutorials/#spatiotemporal-asset-catalogs","title":"SpatioTemporal Asset Catalogs","text":"<ul> <li>STAC Item is the core atomic unit, representing a single spatiotemporal asset as a GeoJSON feature plus datetime and links: https://github.com/radiantearth/stac-spec/blob/master/item-spec/item-spec.md</li> <li>STAC Catalog is a simple, flexible JSON file of links that provides a structure to organize and browse STAC Items. A series of best practices helps make recommendations for creating real world STAC Catalogs: https://github.com/radiantearth/stac-spec/blob/master/catalog-spec/catalog-spec.md</li> <li>STAC Collection is an extension of the STAC Catalog with additional information such as the extents, license, keywords, providers, etc that describe STAC Items that fall within the Collection: https://github.com/radiantearth/stac-spec/blob/master/collection-spec/collection-spec.md</li> <li>STAC API provides a RESTful endpoint that enables search of STAC Items, specified in OpenAPI, following OGC\u2019s WFS 3: https://github.com/radiantearth/stac-api-spec</li> </ul>"},{"location":"oa/usage/tutorials/#tools","title":"Tools","text":"<ul> <li>cwltool the reference reference implementation of the Common Workflow Language standards: https://github.com/common-workflow-language/cwltool</li> <li>calrissian a CWL runner for Kubernetes: https://duke-gcb.github.io/calrissian/</li> </ul>"},{"location":"openeo/admin/configuration/","title":"Configuration","text":"<p>How the BB is configured - with examples etc.</p>"},{"location":"openeo/admin/maintenance/","title":"Maintenance","text":"<p>Administrative and remedial activities to be performed on a running BB instance</p>"},{"location":"openeo/api/endpoint-specification/","title":"Specification","text":"<p>Details of the API specification.</p>"},{"location":"openeo/api/usage/","title":"Usage","text":"<p>API usage descriptions and examples.</p>"},{"location":"openeo/getting-started/more-getting-started/","title":"More getting started\u2026","text":"<p>Further elaboration of Getting Started\u2026</p> <p>Coming soon</p>"},{"location":"openeo/getting-started/quick-start/","title":"Quick Start","text":"<p>Quick start instructions - including installation, e.g. of a local instance.</p> <p>Coming soon</p>"},{"location":"openeo/usage/howtos/","title":"How-Tos","text":"<p>How-tos to communicate usage by example.</p>"},{"location":"openeo/usage/tutorials/","title":"Tutorials","text":"<p>Tutorials as a learning aid.</p>"}]}